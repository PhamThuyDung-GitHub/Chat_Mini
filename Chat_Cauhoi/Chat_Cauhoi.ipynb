{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được xử lý và lưu vào processed_data.csv\n",
      "Dữ liệu đã được xử lý và lưu vào chunked_data.csv\n",
      "Embedding đã được lưu vào embedding_data.csv\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import genai\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import nltk\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import chromadb\n",
    "# import uuid\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # Hàm xóa ký tự đặc biệt\n",
    "# def remove_special_characters(text):\n",
    "#     text = re.sub(r'<.*?>', ' ', text)  # Loại bỏ HTML tags\n",
    "#     text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)  # Loại bỏ URL\n",
    "#     text = re.sub(r'[^\\w\\s.!?@]', ' ', text)  # Loại bỏ ký tự đặc biệt\n",
    "#     return text\n",
    "\n",
    "# # Hàm chuyển chữ về chữ thường\n",
    "# def lowercase(text):\n",
    "#     return text.lower()\n",
    "\n",
    "# # Hàm loại bỏ khoảng trắng thừa\n",
    "# def remove_extra_whitespaces(text):\n",
    "#     text = text.strip()  # Xóa khoảng trắng đầu và cuối\n",
    "#     text = re.sub(r'\\s+', ' ', text)  # Xóa khoảng trắng thừa trong chuỗi\n",
    "#     return text\n",
    "\n",
    "# # Hàm tổng hợp để tiền xử lý văn bản\n",
    "# def preprocess_text(text):\n",
    "#     text = lowercase(text)\n",
    "#     text = remove_special_characters(text)\n",
    "#     text = remove_extra_whitespaces(text)\n",
    "#     return text\n",
    "\n",
    "# # Hàm xóa các dòng trùng lặp dựa trên một cột cụ thể\n",
    "# def remove_duplicate_rows(df, column_name):\n",
    "#     df.drop_duplicates(subset=column_name, keep='first', inplace=True)\n",
    "#     return df\n",
    "\n",
    "# # Hàm tính TF-IDF cho toàn bộ cột \"Câu hỏi\"\n",
    "# def calculate_tfidf(data, column_name):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = vectorizer.fit_transform(data[column_name].dropna().tolist())\n",
    "\n",
    "#     # Lưu TF-IDF vào file CSV\n",
    "#     tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=data[column_name].dropna())\n",
    "#     tfidf_df.to_csv(\"tfidf_values.csv\", index=True)\n",
    "\n",
    "#     return tfidf_matrix\n",
    "\n",
    "# # Hàm chunking semantic để chia đoạn văn\n",
    "# class SemanticChunker:\n",
    "#     def __init__(self, threshold=0.3):\n",
    "#         self.threshold = threshold\n",
    "#         nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "#     def embed_function(self, sentences):\n",
    "#         vectorizer = TfidfVectorizer()\n",
    "#         vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "#         return vectors\n",
    "\n",
    "#     def split_text(self, text):\n",
    "#         sentences = nltk.sent_tokenize(text)  # Tách câu\n",
    "#         sentences = [item for item in sentences if item and item.strip()]\n",
    "#         if not sentences:\n",
    "#             return []\n",
    "\n",
    "#         vectors = self.embed_function(sentences)\n",
    "#         similarities = cosine_similarity(vectors)\n",
    "\n",
    "#         # Lưu cosine similarity vào file CSV\n",
    "#         cosine_df = pd.DataFrame(similarities, index=sentences, columns=sentences)\n",
    "#         cosine_df.to_csv(\"cosine_similarity.csv\", index=True)\n",
    "\n",
    "#         chunks = [[sentences[0]]]  # Bắt đầu chunk đầu tiên\n",
    "#         for i in range(1, len(sentences)):\n",
    "#             sim_score = similarities[i-1, i]\n",
    "#             if sim_score >= self.threshold:\n",
    "#                 chunks[-1].append(sentences[i])\n",
    "#             else:\n",
    "#                 chunks.append([sentences[i]])\n",
    "\n",
    "#         return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# # Hàm chia DataFrame thành các batch\n",
    "# def divide_dataframe(df, batch_size):\n",
    "#     return [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "# # Đọc file CSV đầu vào\n",
    "# input_file = r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\"\n",
    "# output_file = \"processed_data.csv\"\n",
    "# chunked_file = \"chunked_data.csv\"\n",
    "# embedding_file = \"embedding_data.csv\"\n",
    "\n",
    "# try:\n",
    "#     # Đọc dữ liệu từ file CSV\n",
    "#     df = pd.read_csv(input_file)\n",
    "\n",
    "#     # Kiểm tra nếu DataFrame không rỗng\n",
    "#     if not df.empty:\n",
    "#         # Tiền xử lý cột \"Câu hỏi\"\n",
    "#         if 'Câu hỏi' in df.columns:\n",
    "#             df['Câu hỏi'] = df['Câu hỏi'].apply(preprocess_text)\n",
    "\n",
    "#         # Xóa các dòng trùng lặp dựa trên cột 'Câu hỏi' nếu tồn tại\n",
    "#         if 'Câu hỏi' in df.columns:\n",
    "#             df = remove_duplicate_rows(df, 'Câu hỏi')\n",
    "\n",
    "#         # Lưu dữ liệu đã tiền xử lý ra file mới\n",
    "#         df.to_csv(output_file, index=False)\n",
    "#         print(f\"Dữ liệu đã được xử lý và lưu vào {output_file}\")\n",
    "\n",
    "#         # Tính TF-IDF cho cột \"Câu hỏi\"\n",
    "#         tfidf_matrix = calculate_tfidf(df, 'Câu hỏi')\n",
    "\n",
    "#         # Chunking dữ liệu\n",
    "#         chunker = SemanticChunker(threshold=0.3)\n",
    "#         if 'Câu hỏi' in df.columns:\n",
    "#             df['chunk'] = df['Câu hỏi'].apply(lambda x: chunker.split_text(x))\n",
    "\n",
    "#         # Lưu dữ liệu đã chunking ra file mới\n",
    "#         df.to_csv(chunked_file, index=False)\n",
    "#         print(f\"Dữ liệu đã được xử lý và lưu vào {chunked_file}\")\n",
    "\n",
    "#         # Embedding dữ liệu\n",
    "#         embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "#         df['embedding'] = df['chunk'].apply(lambda x: embedding_model.encode(' '.join(x)))\n",
    "\n",
    "#         # Kết nối với Chroma và lưu dữ liệu theo batch\n",
    "#         client = chromadb.PersistentClient(\"db\")\n",
    "#         collection = client.get_or_create_collection(\"embeddings_collection\")\n",
    "#         batch_size = 32\n",
    "#         batches = divide_dataframe(df, batch_size)\n",
    "\n",
    "#         for i, batch in enumerate(batches):\n",
    "#             ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "#             documents = batch['Câu hỏi'].tolist()\n",
    "#             embeddings = batch['embedding'].tolist()\n",
    "#             metadatas = [{\"chunk\": ' '.join(chk)} for chk in batch['chunk']]\n",
    "\n",
    "#             collection.add(\n",
    "#                 ids=ids,\n",
    "#                 documents=documents,\n",
    "#                 embeddings=embeddings,\n",
    "#                 metadatas=metadatas\n",
    "#             )\n",
    "\n",
    "#         # Lưu embedding ra file CSV\n",
    "#         df.to_csv(embedding_file, index=False)\n",
    "#         print(f\"Embedding đã được lưu vào {embedding_file}\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"Dữ liệu đầu vào rỗng!\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Không tìm thấy file {input_file}!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Đã xảy ra lỗi: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được xử lý và lưu vào processed_data.csv\n",
      "Dữ liệu đã được xử lý và lưu vào chunked_data.csv\n",
      "Batch 1/1\n",
      "{'chunk': 'trường đại học công nghệ thông tin dhqg tp.hcm được thành lập từ khi nào?', 'Question': 'trường đại học công nghệ thông tin dhqg tp.hcm được thành lập từ khi nào?', 'Answer': 'Trường Đại Học Công Nghệ Thông Tin DHQG TP.HCM được thành lập từ ngày 8 tháng 6 năm 2006. Trường Đại học Công nghệ Thông tin (UIT), Đại học Quốc gia Thành phố Hồ Chí Minh, được thành lập vào ngày 8 tháng 6 năm 2006 theo quyết định của Thủ tướng Chính phủ.'}\n",
      "{'chunk': 'trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?', 'Question': 'trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?', 'Answer': 'Trường Đại học Công nghệ Thông tin (UIT) đã đạt được nhiều thành tựu nổi bật trong lĩnh vực đào tạo, bao gồm:\\r\\n\\r\\nChất lượng đào tạo: UIT luôn nằm trong top các trường đại học hàng đầu về công nghệ thông tin tại Việt Nam. Chương trình đào tạo của trường được thiết kế theo chuẩn quốc tế, giúp sinh viên có kiến thức vững vàng và kỹ năng thực tiễn.\\r\\nGiải thưởng và thành tích: Sinh viên UIT thường xuyên đạt giải cao trong các cuộc thi quốc gia và quốc tế như Olympic Tin học, ACM/ICPC, và các cuộc thi về an ninh mạng.\\r\\nHợp tác quốc tế: UIT có nhiều chương trình hợp tác với các trường đại học và tổ chức quốc tế, giúp sinh viên có cơ hội học tập và nghiên cứu ở nước ngoài.\\r\\nCơ sở vật chất hiện đại: Trường đầu tư mạnh vào cơ sở vật chất, phòng thí nghiệm, và các trang thiết bị hiện đại để hỗ trợ quá trình học tập và nghiên cứu của sinh viên.\\r\\nĐội ngũ giảng viên: UIT có đội ngũ giảng viên giàu kinh nghiệm, nhiều người trong số họ đã từng học tập và làm việc tại các trường đại học danh tiếng trên thế giới.'}\n",
      "Embedding đã được lưu vào embedding_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import genai\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Hàm xóa ký tự đặc biệt\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Loại bỏ HTML tags\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!\\*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)  # Loại bỏ URL\n",
    "    text = re.sub(r'[^\\w\\s.!?@]', ' ', text)  # Loại bỏ ký tự đặc biệt\n",
    "    return text\n",
    "\n",
    "# Hàm chuyển chữ về chữ thường\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Hàm loại bỏ khoảng trắng thừa\n",
    "def remove_extra_whitespaces(text):\n",
    "    text = text.strip()  # Xóa khoảng trắng đầu và cuối\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Xóa khoảng trắng thừa trong chuỗi\n",
    "    return text\n",
    "\n",
    "# Hàm tổng hợp để tiền xử lý văn bản\n",
    "def preprocess_text(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "# Hàm xóa các dòng trùng lặp dựa trên một cột cụ thể\n",
    "def remove_duplicate_rows(df, column_name):\n",
    "    df.drop_duplicates(subset=column_name, keep='first', inplace=True)\n",
    "    return df\n",
    "\n",
    "# Hàm tính TF-IDF cho toàn bộ cột \"Câu hỏi\"\n",
    "def calculate_tfidf(data, column_name):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(data[column_name].dropna().tolist())\n",
    "\n",
    "    # Lưu TF-IDF vào file CSV\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=data[column_name].dropna())\n",
    "    tfidf_df.to_csv(\"tfidf_values.csv\", index=True)\n",
    "\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Hàm chunking semantic để chia đoạn văn\n",
    "class SemanticChunker:\n",
    "    def __init__(self, threshold=0.3):\n",
    "        self.threshold = threshold\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "    def embed_function(self, sentences):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "        return vectors\n",
    "\n",
    "    def split_text(self, text):\n",
    "        sentences = nltk.sent_tokenize(text)  # Tách câu\n",
    "        sentences = [item for item in sentences if item and item.strip()]\n",
    "        if not sentences:\n",
    "            return []\n",
    "\n",
    "        vectors = self.embed_function(sentences)\n",
    "        similarities = cosine_similarity(vectors)\n",
    "\n",
    "        # Lưu cosine similarity vào file CSV\n",
    "        cosine_df = pd.DataFrame(similarities, index=sentences, columns=sentences)\n",
    "        cosine_df.to_csv(\"cosine_similarity.csv\", index=True)\n",
    "\n",
    "        chunks = [[sentences[0]]]  # Bắt đầu chunk đầu tiên\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim_score = similarities[i-1, i]\n",
    "            if sim_score >= self.threshold:\n",
    "                chunks[-1].append(sentences[i])\n",
    "            else:\n",
    "                chunks.append([sentences[i]])\n",
    "\n",
    "        return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# Hàm chia DataFrame thành các batch\n",
    "def divide_dataframe(df, batch_size):\n",
    "    return [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "# Đọc file CSV đầu vào\n",
    "input_file = r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\"\n",
    "output_file = \"processed_data.csv\"\n",
    "chunked_file = \"chunked_data.csv\"\n",
    "embedding_file = \"embedding_data.csv\"\n",
    "\n",
    "try:\n",
    "    # Đọc dữ liệu từ file CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Kiểm tra nếu DataFrame không rỗng\n",
    "    if not df.empty:\n",
    "        # Tiền xử lý cột \"Câu hỏi\"\n",
    "        if 'Câu hỏi' in df.columns:\n",
    "            df['Câu hỏi'] = df['Câu hỏi'].apply(preprocess_text)\n",
    "\n",
    "        # Xóa các dòng trùng lặp dựa trên cột 'Câu hỏi' nếu tồn tại\n",
    "        if 'Câu hỏi' in df.columns:\n",
    "            df = remove_duplicate_rows(df, 'Câu hỏi')\n",
    "\n",
    "        # Lưu dữ liệu đã tiền xử lý ra file mới\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Dữ liệu đã được xử lý và lưu vào {output_file}\")\n",
    "\n",
    "        # Tính TF-IDF cho cột \"Câu hỏi\"\n",
    "        tfidf_matrix = calculate_tfidf(df, 'Câu hỏi')\n",
    "\n",
    "        # Chunking dữ liệu\n",
    "        chunker = SemanticChunker(threshold=0.3)\n",
    "        if 'Câu hỏi' in df.columns:\n",
    "            df['chunk'] = df['Câu hỏi'].apply(lambda x: chunker.split_text(x))\n",
    "\n",
    "        # Lưu dữ liệu đã chunking ra file mới\n",
    "        df.to_csv(chunked_file, index=False)\n",
    "        print(f\"Dữ liệu đã được xử lý và lưu vào {chunked_file}\")\n",
    "\n",
    "        # Embedding dữ liệu\n",
    "        embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "        df['embedding'] = df['chunk'].apply(lambda x: embedding_model.encode(' '.join(x)))\n",
    "\n",
    "        # Kết nối với Chroma và lưu dữ liệu theo batch\n",
    "        client = chromadb.PersistentClient(\"db\")\n",
    "        collection = client.get_or_create_collection(\"embeddings_collection\")\n",
    "        batch_size = 32\n",
    "        batches = divide_dataframe(df, batch_size)\n",
    "\n",
    "        for i, batch in enumerate(batches):\n",
    "            ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "            documents = batch['Câu hỏi'].tolist()\n",
    "            embeddings = batch['embedding'].tolist()\n",
    "            metadatas = [\n",
    "                {\n",
    "                    \"chunk\": ' '.join(chk),\n",
    "                    \"Question\": question,\n",
    "                    \"Answer\": answer\n",
    "                }\n",
    "                for chk, question, answer in zip(batch['chunk'], batch['Câu hỏi'], batch['Câu trả lời'])\n",
    "            ]\n",
    "\n",
    "            collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "\n",
    "            # Hiển thị metadata trong batch\n",
    "            print(f\"Batch {i + 1}/{len(batches)}\")\n",
    "            for metadata in metadatas:\n",
    "                print(metadata)\n",
    "\n",
    "        # Lưu embedding ra file CSV\n",
    "        df.to_csv(embedding_file, index=False)\n",
    "        print(f\"Embedding đã được lưu vào {embedding_file}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Dữ liệu đầu vào rỗng!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Không tìm thấy file {input_file}!\")\n",
    "except Exception as e:\n",
    "    print(f\"Đã xảy ra lỗi: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả Vector Search:\n",
      "\n",
      "1) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "2) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "3) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "4) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "5) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "6) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "7) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "8) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "9) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "10) Distances: 117.0202 chunk: trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\n",
      "\n",
      "Phản hồi từ Gemini:\n",
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Tr\\u01b0\\u1eddng \\u0110\\u1ea1i h\\u1ecdc C\\u00f4ng ngh\\u1ec7 Th\\u00f4ng tin (UIT) thu\\u1ed9c \\u0110\\u1ea1i h\\u1ecdc Qu\\u1ed1c gia Th\\u00e0nh ph\\u1ed1 H\\u1ed3 Ch\\u00ed Minh c\\u00f3 nhi\\u1ec1u th\\u00e0nh t\\u1ef1u n\\u1ed5i b\\u1eadt trong l\\u0129nh v\\u1ef1c \\u0111\\u00e0o t\\u1ea1o.  M\\u1ed9t s\\u1ed1 th\\u00e0nh t\\u1ef1u \\u0111\\u00e1ng k\\u1ec3 bao g\\u1ed3m: ch\\u1ea5t l\\u01b0\\u1ee3ng \\u0111\\u00e0o t\\u1ea1o cao \\u0111\\u01b0\\u1ee3c ch\\u1ee9ng minh qua t\\u1ef7 l\\u1ec7 sinh vi\\u00ean t\\u1ed1t nghi\\u1ec7p c\\u00f3 vi\\u1ec7c l\\u00e0m cao v\\u00e0 \\u0111\\u01b0\\u1ee3c c\\u00e1c doanh nghi\\u1ec7p c\\u00f4ng ngh\\u1ec7 th\\u00f4ng tin trong v\\u00e0 ngo\\u00e0i n\\u01b0\\u1edbc \\u0111\\u00e1nh gi\\u00e1 cao; ch\\u01b0\\u01a1ng tr\\u00ecnh \\u0111\\u00e0o t\\u1ea1o c\\u1eadp nh\\u1eadt li\\u00ean t\\u1ee5c, \\u0111\\u00e1p \\u1ee9ng nhu c\\u1ea7u nh\\u00e2n l\\u1ef1c c\\u00f4ng ngh\\u1ec7 4.0; \\u0111\\u1ed9i ng\\u0169 gi\\u1ea3ng vi\\u00ean gi\\u00e0u kinh nghi\\u1ec7m, c\\u00f3 tr\\u00ecnh \\u0111\\u1ed9 chuy\\u00ean m\\u00f4n cao v\\u00e0 nhi\\u1ec1u n\\u0103m kinh nghi\\u1ec7m trong ng\\u00e0nh; c\\u01a1 s\\u1edf v\\u1eadt ch\\u1ea5t hi\\u1ec7n \\u0111\\u1ea1i, ph\\u1ee5c v\\u1ee5 t\\u1ed1t cho vi\\u1ec7c h\\u1ecdc t\\u1eadp v\\u00e0 nghi\\u00ean c\\u1ee9u;  h\\u1ee3p t\\u00e1c qu\\u1ed1c t\\u1ebf r\\u1ed9ng r\\u00e3i, t\\u1ea1o \\u0111i\\u1ec1u ki\\u1ec7n cho sinh vi\\u00ean tham gia c\\u00e1c ch\\u01b0\\u01a1ng tr\\u00ecnh trao \\u0111\\u1ed5i, h\\u1ecdc t\\u1eadp v\\u00e0 nghi\\u00ean c\\u1ee9u \\u1edf n\\u01b0\\u1edbc ngo\\u00e0i;  v\\u00e0 cu\\u1ed1i c\\u00f9ng l\\u00e0 vi\\u1ec7c li\\u00ean t\\u1ee5c \\u0111\\u1ea1t \\u0111\\u01b0\\u1ee3c c\\u00e1c gi\\u1ea3i th\\u01b0\\u1edfng v\\u00e0 x\\u1ebfp h\\u1ea1ng cao trong c\\u00e1c b\\u1ea3ng x\\u1ebfp h\\u1ea1ng \\u0111\\u1ea1i h\\u1ecdc trong v\\u00e0 ngo\\u00e0i n\\u01b0\\u1edbc.  T\\u00f3m l\\u1ea1i, UIT kh\\u1eb3ng \\u0111\\u1ecbnh v\\u1ecb th\\u1ebf l\\u00e0 m\\u1ed9t trong nh\\u1eefng tr\\u01b0\\u1eddng \\u0111\\u00e0o t\\u1ea1o c\\u00f4ng ngh\\u1ec7 th\\u00f4ng tin h\\u00e0ng \\u0111\\u1ea7u t\\u1ea1i Vi\\u1ec7t Nam.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.13873573465550199\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 460,\n",
      "        \"candidates_token_count\": 235,\n",
      "        \"total_token_count\": 695\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'GenerateContentResponse' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Nếu có mô hình ngôn ngữ lớn (LLM)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Sử dụng Gemini đã tích hợp API key\u001b[39;00m\n\u001b[0;32m    116\u001b[0m llm_model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m metadatas, hyde_results \u001b[38;5;241m=\u001b[39m \u001b[43mhyde_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns_to_answer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_docs_retrieval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_docs_retrieval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodelai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelai\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKết quả HYDE Search:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(hyde_results)\n",
      "Cell \u001b[1;32mIn[11], line 59\u001b[0m, in \u001b[0;36mhyde_search\u001b[1;34m(llm_model, encoder_model, query, collection, columns_to_answer, number_docs_retrieval, num_samples, modelai)\u001b[0m\n\u001b[0;32m     56\u001b[0m hypothetical_documents \u001b[38;5;241m=\u001b[39m generate_hypothetical_documents(llm_model, query, num_samples)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Tính toán embedding trung bình\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m aggregated_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mencode_hypothetical_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothetical_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Truy vấn dữ liệu từ collection\u001b[39;00m\n\u001b[0;32m     62\u001b[0m search_results \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m     63\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39m[aggregated_embedding],\n\u001b[0;32m     64\u001b[0m     n_results\u001b[38;5;241m=\u001b[39mnumber_docs_retrieval\n\u001b[0;32m     65\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 49\u001b[0m, in \u001b[0;36mencode_hypothetical_documents\u001b[1;34m(documents, encoder_model)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_hypothetical_documents\u001b[39m(documents, encoder_model):\n\u001b[1;32m---> 49\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     50\u001b[0m     avg_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(embeddings, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_embedding\n",
      "File \u001b[1;32mc:\\Users\\ungdu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:591\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    590\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 591\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[1;32mc:\\Users\\ungdu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1050\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m            \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ungdu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:445\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    443\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 445\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    446\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    447\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'GenerateContentResponse' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "\n",
    "# Phương pháp tìm kiếm dựa trên vector\n",
    "def vector_search(model, query, collection, columns_to_answer, number_docs_retrieval, modelai=None):\n",
    "    query_embeddings = model.encode([query])\n",
    "\n",
    "    # Lấy kết quả từ collection\n",
    "    search_results = collection.query(\n",
    "        query_embeddings=query_embeddings,\n",
    "        n_results=number_docs_retrieval\n",
    "    )\n",
    "\n",
    "    metadatas = search_results['metadatas']\n",
    "    scores = search_results['distances']\n",
    "\n",
    "    # Chuẩn bị kết quả tìm kiếm\n",
    "    search_result = \"\"\n",
    "    for i, (meta, score) in enumerate(zip(metadatas[0], scores[0]), start=1):\n",
    "        search_result += f\"\\n{i}) Distances: {score:.4f}\"\n",
    "        for column in columns_to_answer:\n",
    "            if column in meta:\n",
    "                search_result += f\" {column}: {meta.get(column)}\"\n",
    "        search_result += \"\\n\"\n",
    "\n",
    "    # Tạo câu trả lời từ Gemini nếu modelai được cung cấp\n",
    "    if modelai:\n",
    "        enhanced_prompt = f\"Viết đoạn văn trả lời câu hỏi: {query} dựa trên kết quả truy vấn: {search_result}\"\n",
    "        ai_response = modelai.generate_content(enhanced_prompt)\n",
    "        search_result += f\"\\nPhản hồi từ Gemini:\\n{ai_response}\"\n",
    "\n",
    "    return metadatas, search_result\n",
    "\n",
    "# Tạo tài liệu giả định từ câu hỏi\n",
    "def generate_hypothetical_documents(model, query, num_samples=10):\n",
    "    hypothetical_docs = []\n",
    "    for _ in range(num_samples):\n",
    "        enhanced_prompt = f\"Write a paragraph that answers the question: {query}\"\n",
    "        response = model.generate_content(enhanced_prompt)\n",
    "        hypothetical_docs.append(response)\n",
    "\n",
    "    return hypothetical_docs\n",
    "\n",
    "# Tính toán embedding trung bình từ tài liệu giả định\n",
    "def encode_hypothetical_documents(documents, encoder_model):\n",
    "    embeddings = [encoder_model.encode([doc])[0] for doc in documents]\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "# Phương pháp HYDE Search\n",
    "def hyde_search(llm_model, encoder_model, query, collection, columns_to_answer, number_docs_retrieval, num_samples=10, modelai=None):\n",
    "    # Tạo tài liệu giả định\n",
    "    hypothetical_documents = generate_hypothetical_documents(llm_model, query, num_samples)\n",
    "\n",
    "    # Tính toán embedding trung bình\n",
    "    aggregated_embedding = encode_hypothetical_documents(hypothetical_documents, encoder_model)\n",
    "\n",
    "    # Truy vấn dữ liệu từ collection\n",
    "    search_results = collection.query(\n",
    "        query_embeddings=[aggregated_embedding],\n",
    "        n_results=number_docs_retrieval\n",
    "    )\n",
    "\n",
    "    search_result = \"\"\n",
    "    metadatas = search_results['metadatas']\n",
    "\n",
    "    for i, meta in enumerate(metadatas[0], start=1):\n",
    "        search_result += f\"\\n{i})\"\n",
    "        for column in columns_to_answer:\n",
    "            if column in meta:\n",
    "                search_result += f\" {column.capitalize()}: {meta.get(column)}\"\n",
    "        search_result += \"\\n\"\n",
    "\n",
    "    # Tạo câu trả lời từ Gemini nếu modelai được cung cấp\n",
    "    if modelai:\n",
    "        enhanced_prompt = f\"Viết đoạn văn trả lời câu hỏi: {query} dựa trên kết quả truy vấn: {search_result}\"\n",
    "        ai_response = modelai.generate_content(enhanced_prompt)\n",
    "        search_result += f\"\\nPhản hồi từ Gemini:\\n{ai_response}\"\n",
    "\n",
    "    return metadatas, search_result\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "if __name__ == \"__main__\":\n",
    "    # Thiết lập mô hình và collection\n",
    "    embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "    client = chromadb.PersistentClient(\"db\")\n",
    "    collection = client.get_collection(\"embeddings_collection\")\n",
    "\n",
    "    # Cấu hình API cho Google Generative AI\n",
    "    os.environ['GOOGLE_API_KEY'] = \"AIzaSyAgOBMLyULtQE6PBI6u6v-bawhlF3UkhNI\"\n",
    "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "    modelai = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    # Câu truy vấn và tham số\n",
    "    query = \"Nhập câu hỏi tìm kiếm của bạn ở đây\"\n",
    "    columns_to_answer = [\"Câu hỏi\", \"chunk\"]\n",
    "    number_docs_retrieval = 10\n",
    "\n",
    "    # Tìm kiếm bằng Vector Search\n",
    "    metadatas, vector_results = vector_search(\n",
    "        model=embedding_model,\n",
    "        query=query,\n",
    "        collection=collection,\n",
    "        columns_to_answer=columns_to_answer,\n",
    "        number_docs_retrieval=number_docs_retrieval,\n",
    "        modelai=modelai\n",
    "    )\n",
    "    print(\"Kết quả Vector Search:\")\n",
    "    print(vector_results)\n",
    "\n",
    "    # Nếu có mô hình ngôn ngữ lớn (LLM)\n",
    "    # Sử dụng Gemini đã tích hợp API key\n",
    "    llm_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    metadatas, hyde_results = hyde_search(\n",
    "        llm_model=llm_model,\n",
    "        encoder_model=embedding_model,\n",
    "        query=query,\n",
    "        collection=collection,\n",
    "        columns_to_answer=columns_to_answer,\n",
    "        number_docs_retrieval=number_docs_retrieval,\n",
    "        num_samples=5,\n",
    "        modelai=modelai\n",
    "    )\n",
    "    print(\"Kết quả HYDE Search:\")\n",
    "    print(hyde_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'chunk': 'trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?'}, {'chunk': 'trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?'}, {'chunk': 'trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?'}]]\n"
     ]
    }
   ],
   "source": [
    "print(metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieved_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mretrieved_data\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieved_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(retrieved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Vector Search\n",
    "# def vector_search(embedding_model, query, collection, columns_to_answer, number_docs_retrieval):\n",
    "#     \"\"\"\n",
    "#     Perform vector-based search using embeddings.\n",
    "#     \"\"\"\n",
    "#     # Encode the query into embeddings\n",
    "#     query_embeddings = embedding_model.encode([query])\n",
    "    \n",
    "#     # Query the vector database collection\n",
    "#     search_results = collection.query(\n",
    "#         query_embeddings=query_embeddings, \n",
    "#         n_results=number_docs_retrieval\n",
    "#     )  \n",
    "    \n",
    "#     metadatas = search_results['metadatas']  \n",
    "#     scores = search_results['distances']  \n",
    "\n",
    "#     # Prepare the search result output\n",
    "#     search_result = \"\"\n",
    "#     for i, (meta, score) in enumerate(zip(metadatas[0], scores[0]), start=1):\n",
    "#         search_result += f\"\\n{i}) Distances: {score:.4f}\"  \n",
    "#         for column in columns_to_answer:\n",
    "#             if column in meta:\n",
    "#                 search_result += f\" {column}: {meta.get(column)}\"\n",
    "#         search_result += \"\\n\"\n",
    "\n",
    "#     return metadatas, search_result\n",
    "\n",
    "# # HYDE Search (Hypothetical Document Embedding Search)\n",
    "# def hyde_search(llm_model, encoder_model, query, collection, columns_to_answer, number_docs_retrieval, num_samples=10):\n",
    "#     \"\"\"\n",
    "#     Perform HYDE search using hypothetical document embeddings generated by Gemini.\n",
    "#     \"\"\"\n",
    "#     # Generate hypothetical documents using the LLM model\n",
    "#     hypothetical_documents = generate_hypothetical_documents(llm_model, query, num_samples)\n",
    "\n",
    "#     # Encode the hypothetical documents into embeddings\n",
    "#     aggregated_embedding = encode_hypothetical_documents(hypothetical_documents, encoder_model)\n",
    "\n",
    "#     # Query the vector database collection\n",
    "#     search_results = collection.query(\n",
    "#         query_embeddings=[aggregated_embedding], \n",
    "#         n_results=number_docs_retrieval\n",
    "#     )\n",
    "    \n",
    "#     # Prepare the search result output\n",
    "#     search_result = \"\"\n",
    "#     metadatas = search_results['metadatas']\n",
    "#     scores = search_results['distances']\n",
    "\n",
    "#     for i, (meta, score) in enumerate(zip(metadatas[0], scores[0]), start=1):\n",
    "#         search_result += f\"\\n{i}) Distances: {score:.4f}\"  \n",
    "#         for column in columns_to_answer:\n",
    "#             if column in meta:\n",
    "#                 search_result += f\" {column.capitalize()}: {meta.get(column)}\"\n",
    "#         search_result += \"\\n\"\n",
    "    \n",
    "#     return metadatas, search_result\n",
    "\n",
    "# # Generate hypothetical documents using Gemini\n",
    "# def generate_hypothetical_documents(model, query, num_samples=10):\n",
    "#     \"\"\"\n",
    "#     Generate hypothetical documents based on the query using Gemini.\n",
    "#     \"\"\"\n",
    "#     hypothetical_docs = []\n",
    "#     for _ in range(num_samples):\n",
    "#         enhanced_prompt = f\"Write a paragraph that answers the question: {query}\"\n",
    "#         response = model.generate_content(enhanced_prompt)\n",
    "#         hypothetical_docs.append(response)\n",
    "    \n",
    "#     return hypothetical_docs\n",
    "\n",
    "# # Encode hypothetical documents into embeddings\n",
    "# def encode_hypothetical_documents(documents, encoder_model):\n",
    "#     \"\"\"\n",
    "#     Encode a list of documents into embeddings and calculate the average embedding.\n",
    "#     \"\"\"\n",
    "#     embeddings = [encoder_model.encode([doc])[0] for doc in documents]\n",
    "#     avg_embedding = np.mean(embeddings, axis=0)\n",
    "#     return avg_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import genai\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import nltk\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import google.generativeai as genai\n",
    "# import chromadb\n",
    "# import uuid\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# # # Thiết lập API Key cho Google Gemini\n",
    "# # os.environ['GOOGLE_API_KEY'] = \"AIzaSyAgOBMLyULtQE6PBI6u6v-bawhlF3UkhNI\"\n",
    "# # genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "# # modelai = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# # genai.configure(api_key=\"AIzaSyAgOBMLyULtQE6PBI6u6v-bawhlF3UkhNI\")\n",
    "# # modelai = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "\n",
    "# # Hàm xóa ký tự đặc biệt\n",
    "# def remove_special_characters(text):\n",
    "#     text = re.sub(r'<.*?>', ' ', text)  # Loại bỏ HTML tags\n",
    "#     text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!\\*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)  # Loại bỏ URL\n",
    "#     text = re.sub(r'[^\\w\\s.!?@]', ' ', text)  # Loại bỏ ký tự đặc biệt\n",
    "#     return text\n",
    "\n",
    "# # Hàm chuyển chữ về chữ thường\n",
    "# def lowercase(text):\n",
    "#     return text.lower()\n",
    "\n",
    "# # Hàm loại bỏ khoảng trắng thừa\n",
    "# def remove_extra_whitespaces(text):\n",
    "#     text = text.strip()  # Xóa khoảng trắng đầu và cuối\n",
    "#     text = re.sub(r'\\s+', ' ', text)  # Xóa khoảng trắng thừa trong chuỗi\n",
    "#     return text\n",
    "\n",
    "# # Hàm tổng hợp để tiền xử lý văn bản\n",
    "# def preprocess_text(text):\n",
    "#     text = lowercase(text)\n",
    "#     text = remove_special_characters(text)\n",
    "#     text = remove_extra_whitespaces(text)\n",
    "#     return text\n",
    "\n",
    "# # Hàm xóa các dòng trùng lặp dựa trên một cột cụ thể\n",
    "# def remove_duplicate_rows(df, column_name):\n",
    "#     df.drop_duplicates(subset=column_name, keep='first', inplace=True)\n",
    "#     return df\n",
    "\n",
    "# # Hàm tính TF-IDF cho toàn bộ cột \"Câu hỏi\"\n",
    "# def calculate_tfidf(data, column_name):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = vectorizer.fit_transform(data[column_name].dropna().tolist())\n",
    "\n",
    "#     # Lưu TF-IDF vào file CSV\n",
    "#     tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=data[column_name].dropna())\n",
    "#     tfidf_df.to_csv(\"tfidf_values.csv\", index=True)\n",
    "\n",
    "#     return tfidf_matrix\n",
    "\n",
    "# # Hàm chunking semantic để chia đoạn văn\n",
    "# class SemanticChunker:\n",
    "#     def __init__(self, threshold=0.3):\n",
    "#         self.threshold = threshold\n",
    "#         nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "#     def embed_function(self, sentences):\n",
    "#         vectorizer = TfidfVectorizer()\n",
    "#         vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "#         return vectors\n",
    "\n",
    "#     def split_text(self, text):\n",
    "#         sentences = nltk.sent_tokenize(text)  # Tách câu\n",
    "#         sentences = [item for item in sentences if item and item.strip()]\n",
    "#         if not sentences:\n",
    "#             return []\n",
    "\n",
    "#         vectors = self.embed_function(sentences)\n",
    "#         similarities = cosine_similarity(vectors)\n",
    "\n",
    "#         # Lưu cosine similarity vào file CSV\n",
    "#         cosine_df = pd.DataFrame(similarities, index=sentences, columns=sentences)\n",
    "#         cosine_df.to_csv(\"cosine_similarity.csv\", index=True)\n",
    "\n",
    "#         chunks = [[sentences[0]]]  # Bắt đầu chunk đầu tiên\n",
    "#         for i in range(1, len(sentences)):\n",
    "#             sim_score = similarities[i-1, i]\n",
    "#             if sim_score >= self.threshold:\n",
    "#                 chunks[-1].append(sentences[i])\n",
    "#             else:\n",
    "#                 chunks.append([sentences[i]])\n",
    "\n",
    "#         return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# # Hàm chia DataFrame thành các batch\n",
    "# def divide_dataframe(df, batch_size):\n",
    "#     return [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "# # Vector Search\n",
    "# def vector_search(model, query, collection, columns_to_answer, number_docs_retrieval):\n",
    "#     query_embeddings = model.encode([query])\n",
    "#     search_results = collection.query(\n",
    "#         query_embeddings=query_embeddings, \n",
    "#         n_results=number_docs_retrieval\n",
    "#     )\n",
    "#     metadatas = search_results['metadatas']\n",
    "#     scores = search_results['distances']\n",
    "\n",
    "#     search_result = \"\"\n",
    "#     for i, (meta, score) in enumerate(zip(metadatas[0], scores[0]), start=1):\n",
    "#         search_result += f\"\\n{i}) Distances: {score:.4f}\"\n",
    "#         for column in columns_to_answer:\n",
    "#             if column in meta:\n",
    "#                 search_result += f\" {column}: {meta.get(column)}\"\n",
    "#         search_result += \"\\n\"\n",
    "\n",
    "#     return metadatas, search_result\n",
    "\n",
    "# # Generate Hypothetical Documents\n",
    "# def generate_hypothetical_documents(model, query, num_samples=5):\n",
    "#     hypothetical_docs = []\n",
    "#     for _ in range(num_samples):\n",
    "#         response = model.generate_content(\n",
    "#             prompt=f\"Write a paragraph answering the question: {query}\",\n",
    "#             max_tokens=150\n",
    "#         )\n",
    "#         hypothetical_docs.append(response.text.strip())\n",
    "#     return hypothetical_docs\n",
    "\n",
    "# # Encode Hypothetical Documents\n",
    "# def encode_hypothetical_documents(documents, encoder_model):\n",
    "#     embeddings = [encoder_model.encode(doc) for doc in documents]\n",
    "#     avg_embedding = np.mean(embeddings, axis=0)\n",
    "#     return avg_embedding\n",
    "\n",
    "# # HYDE Search\n",
    "# def hyde_search(llm_model, encoder_model, query, collection, columns_to_answer, number_docs_retrieval, num_samples=5):\n",
    "#     hypothetical_documents = generate_hypothetical_documents(llm_model, query, num_samples)\n",
    "#     aggregated_embedding = encode_hypothetical_documents(hypothetical_documents, encoder_model)\n",
    "\n",
    "#     search_results = collection.query(\n",
    "#         query_embeddings=[aggregated_embedding], \n",
    "#         n_results=number_docs_retrieval\n",
    "#     )\n",
    "\n",
    "#     search_result = \"\"\n",
    "#     metadatas = search_results['metadatas']\n",
    "#     scores = search_results['distances']\n",
    "\n",
    "#     for i, (meta, score) in enumerate(zip(metadatas[0], scores[0]), start=1):\n",
    "#         search_result += f\"\\n{i}) Distances: {score:.4f}\"\n",
    "#         for column in columns_to_answer:\n",
    "#             if column in meta:\n",
    "#                 search_result += f\" {column.capitalize()}: {meta.get(column)}\"\n",
    "#         search_result += \"\\n\"\n",
    "\n",
    "#     return search_result\n",
    "\n",
    "\n",
    "# # Đọc file CSV đầu vào\n",
    "# input_file = r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\"\n",
    "# output_file = \"processed_data.csv\"\n",
    "# chunked_file = \"chunked_data.csv\"\n",
    "# embedding_file = \"embedding_data.csv\"\n",
    "\n",
    "# try:\n",
    "#     # Đọc dữ liệu từ file CSV\n",
    "#     df = pd.read_csv(input_file)\n",
    "\n",
    "#     # Kiểm tra nếu DataFrame không rỗng\n",
    "#     if not df.empty:\n",
    "#         # Tiền xử lý cột \"Câu hỏi\"\n",
    "#         if 'Câu hỏi' in df.columns:\n",
    "#             df['Câu hỏi'] = df['Câu hỏi'].apply(preprocess_text)\n",
    "\n",
    "#         # Xóa các dòng trùng lặp dựa trên cột 'Câu hỏi' nếu tồn tại\n",
    "#         if 'Câu hỏi' in df.columns:\n",
    "#             df = remove_duplicate_rows(df, 'Câu hỏi')\n",
    "\n",
    "#         # Lưu dữ liệu đã tiền xử lý ra file mới\n",
    "#         df.to_csv(output_file, index=False)\n",
    "#         print(f\"Dữ liệu đã được xử lý và lưu vào {output_file}\")\n",
    "\n",
    "#         # Tính TF-IDF cho cột \"Câu hỏi\"\n",
    "#         tfidf_matrix = calculate_tfidf(df, 'Câu hỏi')\n",
    "\n",
    "#         # Chunking dữ liệu\n",
    "#         chunker = SemanticChunker(threshold=0.3)\n",
    "#         if 'Câu hỏi' in df.columns:\n",
    "#             df['chunk'] = df['Câu hỏi'].apply(lambda x: chunker.split_text(x))\n",
    "\n",
    "#         # Lưu dữ liệu đã chunking ra file mới\n",
    "#         df.to_csv(chunked_file, index=False)\n",
    "#         print(f\"Dữ liệu đã được xử lý và lưu vào {chunked_file}\")\n",
    "\n",
    "#         # Embedding dữ liệu\n",
    "#         embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "#         df['embedding'] = df['chunk'].apply(lambda x: embedding_model.encode(' '.join(x)))\n",
    "\n",
    "#         # Kết nối với Chroma và lưu dữ liệu theo batch\n",
    "#         client = chromadb.PersistentClient(\"db\")\n",
    "#         collection = client.get_or_create_collection(\"embeddings_collection\")\n",
    "#         batch_size = 256\n",
    "#         batches = divide_dataframe(df, batch_size)\n",
    "\n",
    "#         for i, batch in enumerate(batches):\n",
    "#             ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "#             documents = batch['Câu hỏi'].tolist()\n",
    "#             embeddings = batch['embedding'].tolist()\n",
    "#             metadatas = [{\"chunk\": ' '.join(chk)} for chk in batch['chunk']]\n",
    "\n",
    "#             collection.add(\n",
    "#                 ids=ids,\n",
    "#                 documents=documents,\n",
    "#                 embeddings=embeddings,\n",
    "#                 metadatas=metadatas\n",
    "#             )\n",
    "\n",
    "#         # Lưu embedding ra file CSV\n",
    "#         df.to_csv(embedding_file, index=False)\n",
    "#         print(f\"Embedding đã được lưu vào {embedding_file}\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"Dữ liệu đầu vào rỗng!\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Không tìm thấy file {input_file}!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Đã xảy ra lỗi: {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieved_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Thử nghiệm prompt với Vector Search và Hyde Search\u001b[39;00m\n\u001b[0;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrường UIT Đại học Công nghệ thông tin ĐHQG TP.HCM có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m enhanced_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCâu hỏi của người dùng là: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Trả lời câu hỏi của người dùng dựa trên các dữ liệu sau: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(prompt, \u001b[43mretrieved_data\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieved_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Thử nghiệm prompt với Vector Search và Hyde Search\n",
    "prompt = \"trường UIT Đại học Công nghệ thông tin ĐHQG TP.HCM có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\"\n",
    "enhanced_prompt = \"\"\"Câu hỏi của người dùng là: \"{}\". Trả lời câu hỏi của người dùng dựa trên các dữ liệu sau: \\n{}\"\"\".format(prompt, retrieved_data)\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAsn_IYdisDZuwfpJVfKRoRvnasoZ9h5DM\"\n",
    "\n",
    "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
    "modelai = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "response = modelai.generate_content(enhanced_prompt)\n",
    "\n",
    "response.text\n",
    "\n",
    "\n",
    "# Kiểm tra tìm kiếm Vector Search\n",
    "if embedding_model and collection:\n",
    "    metadatas_vector, retrieved_data_vector = vector_search(\n",
    "        embedding_model,\n",
    "        prompt,\n",
    "        collection,\n",
    "        columns_to_answer=[\"chunk\"],\n",
    "        number_docs_retrieval=5\n",
    "    )\n",
    "    print(\"Vector Search Result:\")\n",
    "    print(retrieved_data_vector)\n",
    "else:\n",
    "    print(\"Vector Search: embedding_model or collection not initialized.\")\n",
    "\n",
    "# Kiểm tra tìm kiếm Hyde Search\n",
    "if modelai and embedding_model and collection:\n",
    "    retrieved_data_hyde = hyde_search_with_gemini(\n",
    "        embedding_model,\n",
    "        prompt,\n",
    "        collection,\n",
    "        columns_to_answer=[\"chunk\"],\n",
    "        number_docs_retrieval=5,\n",
    "        num_samples=3  # Số mẫu tài liệu giả định\n",
    "    )\n",
    "    print(\"\\nHyde Search Result:\")\n",
    "    print(retrieved_data_hyde)\n",
    "else:\n",
    "    print(\"Hyde Search: modelai, embedding_model, or collection not initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid model name: 'gemini-1.5-pro'. Model names should start with 'models/' or 'tunedModels/'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIzaSyAsn_IYdisDZuwfpJVfKRoRvnasoZ9h5DM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m modelai \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-1.5-pro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Prompt cần kiểm tra\u001b[39;00m\n\u001b[0;32m      9\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrường UIT Đại học Công nghệ Thông Tin ĐHQG TP.HCM có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ungdu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\generativeai\\models.py:55\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(name, client, request_options)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\n\u001b[0;32m     34\u001b[0m     name: model_types\u001b[38;5;241m.\u001b[39mAnyModelNameOptions,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m     36\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m     request_options: helper_types\u001b[38;5;241m.\u001b[39mRequestOptionsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m model_types\u001b[38;5;241m.\u001b[39mModel \u001b[38;5;241m|\u001b[39m model_types\u001b[38;5;241m.\u001b[39mTunedModel:\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the API to fetch a model by name.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m        A `types.Model`\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_base_model(name, client\u001b[38;5;241m=\u001b[39mclient, request_options\u001b[38;5;241m=\u001b[39mrequest_options)\n",
      "File \u001b[1;32mc:\\Users\\ungdu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\generativeai\\types\\model_types.py:365\u001b[0m, in \u001b[0;36mmake_model_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type. Expected one of the following types: `str`, `Model`, or `TunedModel`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m     )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtunedModels/\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model name: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Model names should start with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtunedModels/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m     )\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m name\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid model name: 'gemini-1.5-pro'. Model names should start with 'models/' or 'tunedModels/'."
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Đảm bảo rằng bạn đã cấu hình Google Gemini đúng cách\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAsn_IYdisDZuwfpJVfKRoRvnasoZ9h5DM\"\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "modelai = genai.get_model(\"gemini-1.5-pro\")\n",
    "\n",
    "# Prompt cần kiểm tra\n",
    "prompt = \"Trường UIT Đại học Công nghệ Thông Tin ĐHQG TP.HCM có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\"\n",
    "\n",
    "# Gửi prompt đến API\n",
    "try:\n",
    "    response = modelai.generate_content(\n",
    "        prompt=prompt,\n",
    "        max_tokens=150  # Giới hạn số lượng token trong câu trả lời\n",
    "    )\n",
    "\n",
    "    # Hiển thị kết quả\n",
    "    print(\"Câu trả lời từ Google Gemini:\")\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"Đã xảy ra lỗi: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChatSession', 'GenerationConfig', 'GenerativeModel', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'annotations', 'caching', 'configure', 'create_tuned_model', 'delete_file', 'delete_tuned_model', 'embed_content', 'embed_content_async', 'get_base_model', 'get_file', 'get_model', 'get_operation', 'get_tuned_model', 'list_files', 'list_models', 'list_operations', 'list_tuned_models', 'protos', 'responder', 'string_utils', 'types', 'update_tuned_model', 'upload_file', 'utils']\n"
     ]
    }
   ],
   "source": [
    "print(dir(genai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object list_models at 0x00000230972DDA80>\n"
     ]
    }
   ],
   "source": [
    "models = genai.list_models()\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
      "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
      "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
      "                   'version.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
      "                   'million tokens.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro 001',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-exp-0801',\n",
      "      base_model_id='',\n",
      "      version='exp-0801',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
      "                   'across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-001-tuning',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
      "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
      "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
      "      input_token_limit=16384,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
      "                   'released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
      "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
      "                   'smallest and most cost effective Flash model. Replaced by '\n",
      "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
      "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
      "                   'smallest and most cost effective Flash model. Replaced by '\n",
      "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Experimental',\n",
      "      description='Gemini 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='exp_1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1121',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1114',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Thinking Experimental',\n",
      "      description='Gemini 2.0 Flash Thinking Experimental',\n",
      "      input_token_limit=32767,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Thinking Experimental',\n",
      "      description='Gemini 2.0 Flash Thinking Experimental',\n",
      "      input_token_limit=32767,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/learnlm-1.5-pro-experimental',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='LearnLM 1.5 Pro Experimental',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
      "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
      "      input_token_limit=32767,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "# Lấy danh sách các mô hình có sẵn\n",
    "models = list(genai.list_models())\n",
    "\n",
    "# In danh sách các mô hình\n",
    "print(\"Available Models:\")\n",
    "for model in models:\n",
    "    print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GenerativeModel.generate_content() got an unexpected keyword argument 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 37\u001b[0m\n\u001b[0;32m     30\u001b[0m enhanced_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124mCâu hỏi của người dùng: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124mDựa trên các thông tin sau, hãy trả lời câu hỏi của người dùng một cách chi tiết:\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mretrieved_data\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Sinh câu trả lời từ Google Gemini\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menhanced_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# In kết quả\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt nâng cao:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: GenerativeModel.generate_content() got an unexpected keyword argument 'prompt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Thiết lập API Key và mô hình LLM\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAgOBMLyULtQE6PBI6u6v-bawhlF3UkhNI\"\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "llm_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Kết nối với ChromaDB và tải collection\n",
    "client = chromadb.PersistentClient(\"db\")\n",
    "collection_name = \"uit_data_collection\"  # Tên collection của bạn\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "# Tải mô hình embedding\n",
    "embedding_model = SentenceTransformer(\"keepitreal/vietnamese-sbert\")\n",
    "\n",
    "# Prompt và tìm kiếm\n",
    "prompt = \"trường uit đại học công nghệ thông tin dhqg tp.hcm có những thành tựu gì nổi bật trong lĩnh vực đào tạo?\"\n",
    "\n",
    "# Vector Search\n",
    "query_embedding = embedding_model.encode([prompt])\n",
    "search_results = collection.query(query_embeddings=query_embedding, n_results=5)\n",
    "\n",
    "# Xử lý dữ liệu từ search\n",
    "retrieved_data = \"\\n\".join([f\"- {meta.get('chunk')}\" for meta in search_results[\"metadatas\"][0]])\n",
    "\n",
    "# Tạo prompt nâng cao\n",
    "enhanced_prompt = f\"\"\"\n",
    "Câu hỏi của người dùng: \"{prompt}\".\n",
    "Dựa trên các thông tin sau, hãy trả lời câu hỏi của người dùng một cách chi tiết:\n",
    "{retrieved_data}\n",
    "\"\"\"\n",
    "\n",
    "# Sinh câu trả lời từ Google Gemini\n",
    "response = llm_model.generate_content(prompt=enhanced_prompt, max_tokens=300)\n",
    "\n",
    "# In kết quả\n",
    "print(\"Prompt nâng cao:\")\n",
    "print(enhanced_prompt)\n",
    "print(\"\\nCâu trả lời từ Gemini:\")\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
