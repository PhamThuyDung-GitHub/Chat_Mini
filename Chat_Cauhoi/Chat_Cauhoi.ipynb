{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu được tải thành công!\n",
      "Tiền xử lý dữ liệu hoàn tất.\n",
      "Dữ liệu đã xử lý:\n",
      "                                             câu_hỏi  \\\n",
      "0  Trường Đại Học Công Nghệ Thông Tin DHQG TP.HCM...   \n",
      "1  Trường (UIT) Đại Học Công Nghệ Thông Tin DHQG ...   \n",
      "\n",
      "                                         câu_trả_lời  \n",
      "0  Trường Đại Học Công Nghệ Thông Tin DHQG TP.HCM...  \n",
      "1  Trường Đại học Công nghệ Thông tin (UIT) đã đạ...  \n",
      "Dữ liệu đã được lưu tại processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Hàm đọc file CSV\n",
    "def load_csv(file_path):\n",
    "    \"\"\"\n",
    "    Đọc dữ liệu từ file CSV và trả về DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\")\n",
    "        print(\"Dữ liệu được tải thành công!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "# Hàm tiền xử lý dữ liệu\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Tiền xử lý dữ liệu:\n",
    "    - Loại bỏ các giá trị NULL.\n",
    "    - Chuẩn hóa tên cột.\n",
    "    - Loại bỏ dòng trùng lặp.\n",
    "    \"\"\"\n",
    "    # Loại bỏ giá trị NULL\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Chuẩn hóa tên cột\n",
    "    df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    \n",
    "    # Loại bỏ dòng trùng lặp\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(\"Tiền xử lý dữ liệu hoàn tất.\")\n",
    "    return df\n",
    "\n",
    "# Hàm ghi dữ liệu đã xử lý ra file CSV\n",
    "def save_csv(df, output_path):\n",
    "    \"\"\"\n",
    "    Ghi dữ liệu đã xử lý ra file CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Dữ liệu đã được lưu tại {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file CSV: {e}\")\n",
    "\n",
    "# Hàm chính để kiểm tra với dữ liệu nhỏ\n",
    "def main():\n",
    "    # Đường dẫn file input và output\n",
    "    input_path = \"mini_data.csv\"\n",
    "    output_path = \"processed_data.csv\"\n",
    "    \n",
    "    # Đọc file CSV\n",
    "    df = load_csv(input_path)\n",
    "    if df is not None:\n",
    "        # Tiền xử lý dữ liệu\n",
    "        processed_df = preprocess_data(df)\n",
    "        \n",
    "        # Hiển thị một số dòng dữ liệu đã xử lý\n",
    "        print(\"Dữ liệu đã xử lý:\")\n",
    "        print(processed_df.head())\n",
    "        \n",
    "        # Ghi dữ liệu ra file\n",
    "        save_csv(processed_df, output_path)\n",
    "\n",
    "# Gọi hàm chính\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking hoàn tất.\n",
      "Dữ liệu đã chunked:\n",
      "                                             câu_hỏi  \\\n",
      "0  Trường Đại Học Công Nghệ Thông Tin DHQG TP.HCM...   \n",
      "1  Trường (UIT) Đại Học Công Nghệ Thông Tin DHQG ...   \n",
      "\n",
      "                                              chunks  \n",
      "0  [Trường Đại Học Công Nghệ Thông Tin DHQG TP.HC...  \n",
      "1  [Trường (UIT) Đại Học Công Nghệ Thông Tin DHQG...  \n",
      "Dữ liệu đã được lưu tại chunked_data.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hàm để thực hiện chunking\n",
    "def split_text_into_chunks(text, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Phân đoạn văn bản dựa trên độ tương đồng ngữ nghĩa giữa các câu.\n",
    "    - text: Văn bản đầu vào.\n",
    "    - threshold: Ngưỡng tương đồng để quyết định bắt đầu một chunk mới.\n",
    "    \"\"\"\n",
    "    # Tách văn bản thành các câu\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    # Tạo biểu diễn TF-IDF cho các câu\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    vectors = vectorizer.toarray()\n",
    "    \n",
    "    # Tính độ tương đồng ngữ nghĩa giữa các câu\n",
    "    similarities = cosine_similarity(vectors)\n",
    "    \n",
    "    # Tạo các chunk dựa trên độ tương đồng\n",
    "    chunks = [[sentences[0]]]\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim_score = similarities[i - 1, i]\n",
    "        if sim_score >= threshold:\n",
    "            chunks[-1].append(sentences[i])\n",
    "        else:\n",
    "            chunks.append([sentences[i]])\n",
    "    \n",
    "    # Gộp các câu trong mỗi chunk thành đoạn hoàn chỉnh\n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# Hàm xử lý chunking trên DataFrame\n",
    "def apply_chunking_to_dataframe(df, column, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Áp dụng chunking cho một cột trong DataFrame.\n",
    "    - df: DataFrame đầu vào.\n",
    "    - column: Tên cột cần áp dụng chunking.\n",
    "    - threshold: Ngưỡng tương đồng cho chunking.\n",
    "    \"\"\"\n",
    "    df[\"chunks\"] = df[column].apply(lambda x: split_text_into_chunks(str(x), threshold))\n",
    "    print(\"Chunking hoàn tất.\")\n",
    "    return df\n",
    "\n",
    "# Tích hợp vào hàm chính\n",
    "def main_chunking():\n",
    "    input_path = \"processed_data.csv\"\n",
    "    output_path = \"chunked_data.csv\"\n",
    "    \n",
    "    # Đọc dữ liệu đã xử lý\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # Áp dụng chunking\n",
    "    if \"câu_hỏi\" in df.columns:\n",
    "        chunked_df = apply_chunking_to_dataframe(df, column=\"câu_hỏi\", threshold=0.3)\n",
    "        \n",
    "        # Hiển thị dữ liệu chunked\n",
    "        print(\"Dữ liệu đã chunked:\")\n",
    "        print(chunked_df[[\"câu_hỏi\", \"chunks\"]].head())\n",
    "        \n",
    "        # Lưu dữ liệu đã chunked ra file CSV\n",
    "        chunked_df.to_csv(output_path, index=False)\n",
    "        print(f\"Dữ liệu đã được lưu tại {output_path}\")\n",
    "    else:\n",
    "        print(\"Cột 'câu_hỏi' không tồn tại trong dữ liệu.\")\n",
    "\n",
    "# Gọi hàm chính\n",
    "if __name__ == \"__main__\":\n",
    "    main_chunking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
