{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được xử lý và lưu vào processed_data.csv\n",
      "Dữ liệu đã được xử lý và lưu vào chunked_data.csv\n",
      "Embedding đã được lưu vào embedding_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Hàm xóa ký tự đặc biệt\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Loại bỏ HTML tags\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)  # Loại bỏ URL\n",
    "    text = re.sub(r'[^\\w\\s.!?@]', ' ', text)  # Loại bỏ ký tự đặc biệt\n",
    "    return text\n",
    "\n",
    "# Hàm chuyển chữ về chữ thường\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Hàm loại bỏ khoảng trắng thừa\n",
    "def remove_extra_whitespaces(text):\n",
    "    text = text.strip()  # Xóa khoảng trắng đầu và cuối\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Xóa khoảng trắng thừa trong chuỗi\n",
    "    return text\n",
    "\n",
    "# Hàm tổng hợp để tiền xử lý văn bản\n",
    "def preprocess_text(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "# Hàm xóa các dòng trùng lặp dựa trên một cột cụ thể\n",
    "def remove_duplicate_rows(df, column_name):\n",
    "    df.drop_duplicates(subset=column_name, keep='first', inplace=True)\n",
    "    return df\n",
    "\n",
    "# Hàm chunking semantic để chia đoạn văn\n",
    "class SemanticChunker:\n",
    "    def __init__(self, threshold=0.3):\n",
    "        self.threshold = threshold\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "    def embed_function(self, sentences):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "        # Lưu TF-IDF vào file CSV\n",
    "        tfidf_df = pd.DataFrame(vectors, index=sentences, columns=vectorizer.get_feature_names_out())\n",
    "        tfidf_df.to_csv(\"tfidf_values.csv\", index=True)\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def split_text(self, text):\n",
    "        sentences = nltk.sent_tokenize(text)  # Tách câu\n",
    "        sentences = [item for item in sentences if item and item.strip()]\n",
    "        if not sentences:\n",
    "            return []\n",
    "\n",
    "        vectors = self.embed_function(sentences)\n",
    "        similarities = cosine_similarity(vectors)\n",
    "\n",
    "        # Lưu cosine similarity vào file CSV\n",
    "        cosine_df = pd.DataFrame(similarities, index=sentences, columns=sentences)\n",
    "        cosine_df.to_csv(\"cosine_similarity.csv\", index=True)\n",
    "\n",
    "        chunks = [[sentences[0]]]  # Bắt đầu chunk đầu tiên\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim_score = similarities[i-1, i]\n",
    "            if sim_score >= self.threshold:\n",
    "                chunks[-1].append(sentences[i])\n",
    "            else:\n",
    "                chunks.append([sentences[i]])\n",
    "\n",
    "        return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# Hàm chia DataFrame thành các batch\n",
    "def divide_dataframe(df, batch_size):\n",
    "    return [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "# Đọc file CSV đầu vào\n",
    "input_file = r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\"\n",
    "output_file = \"processed_data.csv\"\n",
    "chunked_file = \"chunked_data.csv\"\n",
    "embedding_file = \"embedding_data.csv\"\n",
    "\n",
    "try:\n",
    "    # Đọc dữ liệu từ file CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Kiểm tra nếu DataFrame không rỗng\n",
    "    if not df.empty:\n",
    "        # Tiền xử lý cột \"Câu hỏi\"\n",
    "        if 'Câu hỏi' in df.columns:\n",
    "            df['Câu hỏi'] = df['Câu hỏi'].apply(preprocess_text)\n",
    "\n",
    "        # Xóa các dòng trùng lặp dựa trên cột 'Câu hỏi' nếu tồn tại\n",
    "        if 'Câu hỏi' in df.columns:\n",
    "            df = remove_duplicate_rows(df, 'Câu hỏi')\n",
    "\n",
    "        # Lưu dữ liệu đã tiền xử lý ra file mới\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Dữ liệu đã được xử lý và lưu vào {output_file}\")\n",
    "\n",
    "        # Chunking dữ liệu\n",
    "        chunker = SemanticChunker(threshold=0.3)\n",
    "        if 'Câu hỏi' in df.columns:\n",
    "            df['chunk'] = df['Câu hỏi'].apply(lambda x: chunker.split_text(x))\n",
    "\n",
    "        # Lưu dữ liệu đã chunking ra file mới\n",
    "        df.to_csv(chunked_file, index=False)\n",
    "        print(f\"Dữ liệu đã được xử lý và lưu vào {chunked_file}\")\n",
    "\n",
    "        # Embedding dữ liệu\n",
    "        embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "        df['embedding'] = df['chunk'].apply(lambda x: embedding_model.encode(' '.join(x)))\n",
    "\n",
    "        # Kết nối với Chroma và lưu dữ liệu theo batch\n",
    "        client = chromadb.PersistentClient(\"db\")\n",
    "        collection = client.get_or_create_collection(\"embeddings_collection\")\n",
    "        batch_size = 256\n",
    "        batches = divide_dataframe(df, batch_size)\n",
    "        \n",
    "        for i, batch in enumerate(batches):\n",
    "            ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "            documents = batch['Câu hỏi'].tolist()\n",
    "            embeddings = batch['embedding'].tolist()\n",
    "            metadatas = [{\"chunk\": ' '.join(chk)} for chk in batch['chunk']]\n",
    "\n",
    "            collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "\n",
    "        # Lưu embedding ra file CSV\n",
    "        df.to_csv(embedding_file, index=False)\n",
    "        print(f\"Embedding đã được lưu vào {embedding_file}\")\n",
    "    else:\n",
    "        print(\"Dữ liệu đầu vào rỗng!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Không tìm thấy file {input_file}!\")\n",
    "except Exception as e:\n",
    "    print(f\"Đã xảy ra lỗi: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cho mình code để tải file csv và tiền xử lý dữ liệu liên quan đến những file code chính này, nhớ đưa tất cả các hàm liên quan để kiểm tra với data nhỏ nha. với input là file mini_data.csv và output thi in ra file processed_data.csv nhá . cho mình bình luận bằng tiếng việt trong code nha và biểu diễn nó ngay tại màn hình này"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cho mình code tiếp theo sau code trên với giai đoạn chunking - tạo các hàm liên quan cho giai đoạn chunking (mới hoàn toàn, ko sd những file code sẵn này, nhớ viết tiếp sau khi code này và tạo lại các hàm liên quan thuộc về sematic chunk đã chạy nhá "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
