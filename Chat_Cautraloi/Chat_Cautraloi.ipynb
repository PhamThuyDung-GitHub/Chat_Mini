{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Hàm xóa ký tự đặc biệt\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Loại bỏ HTML tags\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!\\*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)  # Loại bỏ URL\n",
    "    text = re.sub(r'[^\\w\\s.!?@]', ' ', text)  # Loại bỏ ký tự đặc biệt\n",
    "    return text\n",
    "\n",
    "# Hàm chuyển chữ về chữ thường\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Hàm loại bỏ khoảng trắng thừa\n",
    "def remove_extra_whitespaces(text):\n",
    "    text = text.strip()  # Xóa khoảng trắng đầu và cuối\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Xóa khoảng trắng thừa trong chuỗi\n",
    "    return text\n",
    "\n",
    "# Hàm tổng hợp để tiền xử lý văn bản\n",
    "def preprocess_text(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "# Hàm xóa các dòng trùng lặp dựa trên một cột cụ thể\n",
    "def remove_duplicate_rows(df, column_name):\n",
    "    df.drop_duplicates(subset=column_name, keep='first', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunker:\n",
    "    def __init__(self, threshold=0.3, output_dir=\"output_details\"):\n",
    "        self.threshold = threshold\n",
    "        self.output_dir = output_dir\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "        # Define Vietnamese stop words\n",
    "        self.stop_words = [\n",
    "            \"và\", \"hoặc\", \"nhưng\", \"là\", \"có\", \"một\", \"những\", \"đó\", \"đây\", \"kia\",\n",
    "            \"với\", \"trên\", \"trong\", \"này\", \"nọ\", \"của\", \"cho\", \"từ\", \"để\", \"vì\",\n",
    "            \"khi\", \"bởi\", \"thì\", \"lại\", \"đã\", \"sẽ\", \"rất\", \"cũng\", \"nữa\", \"hơn\",\n",
    "            \"nào\", \"đều\", \"đang\", \"rằng\", \"vẫn\", \"chỉ\", \"cả\", \"tất\", \"vậy\",\n",
    "            \"thế\", \"sao\", \"nên\", \"ra\", \"gì\"\n",
    "        ]\n",
    "\n",
    "    def embed_function(self, sentences, row_index):\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stop_words)\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences).toarray()\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Calculate TF, IDF, and TF-IDF\n",
    "        tf = tfidf_matrix / np.sum(tfidf_matrix, axis=1, keepdims=True)\n",
    "        idf = vectorizer.idf_\n",
    "        tfidf = tf * idf\n",
    "\n",
    "        # Save details to CSV\n",
    "        tf_df = pd.DataFrame(tf, columns=feature_names)\n",
    "        tf_df.to_csv(f\"{self.output_dir}/row_{row_index}_TF.csv\", index=False)\n",
    "\n",
    "        idf_df = pd.DataFrame({\"Term\": feature_names, \"IDF\": idf})\n",
    "        idf_df.to_csv(f\"{self.output_dir}/row_{row_index}_IDF.csv\", index=False)\n",
    "\n",
    "        tfidf_df = pd.DataFrame(tfidf, columns=feature_names)\n",
    "        tfidf_df.to_csv(f\"{self.output_dir}/row_{row_index}_TFIDF.csv\", index=False)\n",
    "\n",
    "        print(f\"Details for row {row_index} saved to {self.output_dir}\")\n",
    "        return tfidf_matrix\n",
    "\n",
    "    def split_text(self, text, row_index):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        print(f\"Sentence Tokenization Result for row {row_index}:\", sentences)\n",
    "\n",
    "        # Save tokenized sentences to CSV\n",
    "        sentences_df = pd.DataFrame({\"Sentences\": sentences})\n",
    "        sentences_df.to_csv(f\"{self.output_dir}/row_{row_index}_Sentences.csv\", index=False)\n",
    "\n",
    "        if not sentences:\n",
    "            return []\n",
    "\n",
    "        vectors = self.embed_function(sentences, row_index)\n",
    "        similarities = cosine_similarity(vectors)\n",
    "\n",
    "        # Save cosine similarity matrix to CSV\n",
    "        similarity_df = pd.DataFrame(similarities, index=sentences, columns=sentences)\n",
    "        similarity_df.to_csv(f\"{self.output_dir}/row_{row_index}_CosineSimilarity.csv\")\n",
    "\n",
    "        print(f\"Cosine Similarity Matrix for row {row_index} saved to {self.output_dir}\")\n",
    "    \n",
    "        chunks = [[sentences[0]]]\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim_score = similarities[i - 1, i]\n",
    "            if sim_score >= self.threshold:\n",
    "                chunks[-1].append(sentences[i])\n",
    "            else:\n",
    "                chunks.append([sentences[i]])\n",
    "\n",
    "        return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "# Hàm chia DataFrame thành các batch\n",
    "def divide_dataframe(df, batch_size):\n",
    "    return [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được xử lý và lưu vào processed_data.csv\n",
      "Sentence Tokenization Result for row 0: ['quả cam ngon.', 'quả táo dở.', 'quả chanh chua.', 'quả mít to.', 'quả mít rất thơm nữa']\n",
      "Details for row 0 saved to C:\\Users\\ungdu\\Downloads\\Chat_Mini\\Answer\n",
      "Cosine Similarity Matrix for row 0 saved to C:\\Users\\ungdu\\Downloads\\Chat_Mini\\Answer\n",
      "Sentence Tokenization Result for row 1: ['quả cam có hình tròn.', 'quả táo có hình tròn hơi nhỏ.', 'quả chanh hình bầu dục.', 'quả mít to dài có vỏ xù xì.', 'quả mít có thể lấy gỗ']\n",
      "Details for row 1 saved to C:\\Users\\ungdu\\Downloads\\Chat_Mini\\Answer\n",
      "Cosine Similarity Matrix for row 1 saved to C:\\Users\\ungdu\\Downloads\\Chat_Mini\\Answer\n",
      "Chunked data saved to chunked_data.csv\n",
      "Embedding đã được lưu vào embedding_data.csv\n",
      "Batch 1/1\n",
      "{'chunk': 'quả cam ngon.', 'Question': 'các quả có mùi vị như thế nào', 'Answer': 'quả cam ngon. quả táo dở. quả chanh chua. quả mít to. quả mít rất thơm nữa'}\n",
      "{'chunk': 'quả táo dở.', 'Question': 'các quả có mùi vị như thế nào', 'Answer': 'quả cam ngon. quả táo dở. quả chanh chua. quả mít to. quả mít rất thơm nữa'}\n",
      "{'chunk': 'quả chanh chua.', 'Question': 'các quả có mùi vị như thế nào', 'Answer': 'quả cam ngon. quả táo dở. quả chanh chua. quả mít to. quả mít rất thơm nữa'}\n",
      "{'chunk': 'quả mít to. quả mít rất thơm nữa', 'Question': 'các quả có mùi vị như thế nào', 'Answer': 'quả cam ngon. quả táo dở. quả chanh chua. quả mít to. quả mít rất thơm nữa'}\n",
      "{'chunk': 'quả cam có hình tròn. quả táo có hình tròn hơi nhỏ.', 'Question': 'các quả có hình dáng như thế nào', 'Answer': 'quả cam có hình tròn. quả táo có hình tròn hơi nhỏ. quả chanh hình bầu dục. quả mít to dài có vỏ xù xì. quả mít có thể lấy gỗ'}\n",
      "{'chunk': 'quả chanh hình bầu dục.', 'Question': 'các quả có hình dáng như thế nào', 'Answer': 'quả cam có hình tròn. quả táo có hình tròn hơi nhỏ. quả chanh hình bầu dục. quả mít to dài có vỏ xù xì. quả mít có thể lấy gỗ'}\n",
      "{'chunk': 'quả mít to dài có vỏ xù xì.', 'Question': 'các quả có hình dáng như thế nào', 'Answer': 'quả cam có hình tròn. quả táo có hình tròn hơi nhỏ. quả chanh hình bầu dục. quả mít to dài có vỏ xù xì. quả mít có thể lấy gỗ'}\n",
      "{'chunk': 'quả mít có thể lấy gỗ', 'Question': 'các quả có hình dáng như thế nào', 'Answer': 'quả cam có hình tròn. quả táo có hình tròn hơi nhỏ. quả chanh hình bầu dục. quả mít to dài có vỏ xù xì. quả mít có thể lấy gỗ'}\n"
     ]
    }
   ],
   "source": [
    "# Đọc file CSV đầu vào\n",
    "input_file = r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\mini_data.csv\"\n",
    "output_file = \"processed_data.csv\"\n",
    "chunked_file = \"chunked_data.csv\"\n",
    "embedding_file = \"embedding_data.csv\"\n",
    "output_dir = r\"C:\\Users\\ungdu\\Downloads\\Chat_Mini\\Answer\" \n",
    "\n",
    "try:\n",
    "    # Đọc dữ liệu từ file CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Kiểm tra nếu DataFrame không rỗng\n",
    "    if not df.empty:\n",
    "        # Tiền xử lý cột \"Câu hỏi\"\n",
    "        if 'Câu hỏi' in df.columns and 'Câu trả lời' in df.columns:\n",
    "            df['Câu hỏi'] = df['Câu hỏi'].apply(preprocess_text)\n",
    "            df['Câu trả lời'] = df['Câu trả lời'].apply(preprocess_text)\n",
    "\n",
    "        # Xóa các dòng trùng lặp dựa trên cột 'Câu hỏi' nếu tồn tại\n",
    "        if 'Câu hỏi' in df.columns and 'Câu trả lời' in df.columns:\n",
    "            df = remove_duplicate_rows(df, 'Câu hỏi')\n",
    "            df = remove_duplicate_rows(df, 'Câu trả lời')\n",
    "\n",
    "        # Lưu dữ liệu đã tiền xử lý ra file mới\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Dữ liệu đã được xử lý và lưu vào {output_file}\")\n",
    "\n",
    "\n",
    "        chunker = SemanticChunker(threshold=0.3, output_dir=output_dir)\n",
    "        chunk_records = []\n",
    "\n",
    "        if 'Câu trả lời' in df.columns:\n",
    "            for index, row in df.iterrows():\n",
    "                selected_text = row['Câu trả lời']\n",
    "                if isinstance(selected_text, str) and selected_text.strip():\n",
    "                    chunks = chunker.split_text(selected_text, row_index=index)\n",
    "                    for chunk in chunks:\n",
    "                        new_record = row.to_dict()\n",
    "                        new_record['chunk'] = chunk\n",
    "                        chunk_records.append(new_record)\n",
    "\n",
    "        chunked_df = pd.DataFrame(chunk_records)\n",
    "        chunked_df.to_csv(chunked_file, index=False)\n",
    "        print(f\"Chunked data saved to {chunked_file}\")\n",
    "\n",
    "        # Embedding dữ liệu\n",
    "        embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "\n",
    "        # Tính toán embedding từ cột chunk trong chunked_df\n",
    "        chunked_df['embedding'] = chunked_df['chunk'].apply(\n",
    "            lambda x: embedding_model.encode(x) if isinstance(x, str) else None\n",
    "        )\n",
    "\n",
    "        # Lưu embedding ra file CSV\n",
    "        chunked_df.to_csv(embedding_file, index=False)\n",
    "        print(f\"Embedding đã được lưu vào {embedding_file}\")\n",
    "\n",
    "        # Kết nối với Chroma và lưu dữ liệu theo batch\n",
    "        client = chromadb.PersistentClient(\"db\")\n",
    "        collection = client.get_or_create_collection(\"embeddings_collection\")\n",
    "        batch_size = 256\n",
    "        batches = divide_dataframe(chunked_df, batch_size)\n",
    "\n",
    "        for i, batch in enumerate(batches):\n",
    "            ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "            documents = batch['chunk'].tolist()\n",
    "            embeddings = batch['embedding'].tolist()\n",
    "            metadatas = [\n",
    "                {\n",
    "                    \"chunk\": chunk,\n",
    "                    \"Question\": question,\n",
    "                    \"Answer\": answer\n",
    "                }\n",
    "                for chunk, question, answer in zip(batch['chunk'], batch['Câu hỏi'], batch['Câu trả lời'])\n",
    "            ]\n",
    "\n",
    "            collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "\n",
    "            # Hiển thị metadata trong batch\n",
    "            print(f\"Batch {i + 1}/{len(batches)}\")\n",
    "            for metadata in metadatas:\n",
    "                print(metadata)\n",
    "\n",
    "    else:\n",
    "        print(\"Dữ liệu đầu vào rỗng!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Không tìm thấy file {input_file}!\")\n",
    "except Exception as e:\n",
    "    print(f\"Đã xảy ra lỗi: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "\n",
    "# Configure Generative AI\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAgOBMLyULtQE6PBI6u6v-bawhlF3UkhNI\"\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "modelai = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Load SentenceTransformer model\n",
    "encoder_model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "\n",
    "# Connect to ChromaDB\n",
    "client = chromadb.PersistentClient(\"db\")\n",
    "collection = client.get_or_create_collection(\"embeddings_collection\")\n",
    "\n",
    "# Verify collection embedding dimension\n",
    "def verify_embedding_dimension(collection, expected_dimension=768):\n",
    "    try:\n",
    "        # Generate a dummy embedding to verify\n",
    "        dummy_embedding = [0.0] * expected_dimension\n",
    "        collection.query(query_embeddings=[dummy_embedding], n_results=1)\n",
    "        print(\"Embedding dimension matches expected size.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Embedding dimension verification failed. Details: {str(e)}\")\n",
    "\n",
    "# Define vector search function\n",
    "def vector_search(query, collection, columns_to_answer, number_docs_retrieval=2):\n",
    "    query_embeddings = encoder_model.encode([query])\n",
    "\n",
    "    if isinstance(query_embeddings, np.ndarray):\n",
    "        query_embeddings = query_embeddings.tolist()  # Convert numpy array to list\n",
    "\n",
    "    search_results = collection.query(\n",
    "        query_embeddings=query_embeddings,\n",
    "        n_results=number_docs_retrieval\n",
    "    )\n",
    "\n",
    "    metadatas = search_results['metadatas']\n",
    "    scores = search_results['distances']\n",
    "\n",
    "    search_result = \"\"\n",
    "    for i, (meta, score) in enumerate(zip(metadatas[0], scores[0]), start=1):\n",
    "        search_result += f\"\\n{i}) Distance: {score:.4f}\"\n",
    "        for column in columns_to_answer:\n",
    "            if column in meta:\n",
    "                search_result += f\" {column.capitalize()}: {meta.get(column)}\"\n",
    "        search_result += \"\\n\"\n",
    "\n",
    "    return metadatas, search_result\n",
    "\n",
    "# Define HYDE-based search function\n",
    "def generate_hypothetical_documents(model, query, num_samples=10):\n",
    "    hypothetical_docs = []\n",
    "    for _ in range(num_samples):\n",
    "        enhanced_prompt = f\"Write a paragraph that answers the question: {query}\"\n",
    "        response = model.generate_content(enhanced_prompt)\n",
    "        if hasattr(response, 'content'):\n",
    "            hypothetical_docs.append(response.content)\n",
    "    return hypothetical_docs\n",
    "\n",
    "def encode_hypothetical_documents(documents, encoder_model):\n",
    "    if not documents:\n",
    "        raise ValueError(\"No hypothetical documents generated. Cannot encode empty documents.\")\n",
    "\n",
    "    embeddings = [encoder_model.encode(doc) for doc in documents]\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "    if isinstance(avg_embedding, np.ndarray):\n",
    "        avg_embedding = avg_embedding.tolist()  # Convert numpy array to list\n",
    "\n",
    "    return [avg_embedding]  # Return as a list of one embedding\n",
    "\n",
    "def hyde_search(encoder_model, query, collection, columns_to_answer, number_docs_retrieval=2, num_samples=2):\n",
    "    hypothetical_documents = generate_hypothetical_documents(modelai, query, num_samples)\n",
    "\n",
    "    print(\"Hypothetical Documents:\", hypothetical_documents)\n",
    "\n",
    "    if not hypothetical_documents:\n",
    "        print(\"No hypothetical documents generated. Skipping HYDE search.\")\n",
    "        return [], \"No hypothetical documents generated.\"\n",
    "\n",
    "    # Encode the hypothetical documents into embeddings\n",
    "    aggregated_embedding = encode_hypothetical_documents(hypothetical_documents, encoder_model)\n",
    "\n",
    "    # Perform the search on the collection with the generated embeddings\n",
    "    search_results = collection.query(\n",
    "        query_embeddings=aggregated_embedding,\n",
    "        n_results=number_docs_retrieval\n",
    "    )\n",
    "\n",
    "    search_result = \"\"\n",
    "    metadatas = search_results['metadatas']\n",
    "\n",
    "    # Format the search results\n",
    "    for i, meta in enumerate(metadatas[0], start=1):\n",
    "        search_result += f\"\\n{i})\"\n",
    "        for column in columns_to_answer:\n",
    "            if column in meta:\n",
    "                search_result += f\" {column.capitalize()}: {meta.get(column)}\"\n",
    "        search_result += \"\\n\"\n",
    "\n",
    "    return metadatas, search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vector Search + LLM ---\n",
      "Retrieved Data: \n",
      "1) Distance: 36.5667 Chunk: quả cam ngon.\n",
      "\n",
      "2) Distance: 36.5667 Chunk: quả cam ngon.\n",
      "\n",
      "LLM Response: Dựa trên dữ liệu cung cấp, chỉ có thông tin về chất lượng của quả cam (\"quả cam ngon\") chứ không có thông tin về hình dạng.  Tuy nhiên, dựa trên kiến thức chung về quả cam, **quả cam thường có hình cầu hoặc hình hơi bầu dục**.  Không có dữ liệu nào trong các đoạn văn bản cho phép xác định hình dạng chính xác hơn.\n",
      "\n",
      "Hypothetical Documents: []\n",
      "No hypothetical documents generated. Skipping HYDE search.\n",
      "\n",
      "--- HYDE Search + LLM ---\n",
      "Retrieved Data: No hypothetical documents generated.\n",
      "LLM Response: Quả cam thường có hình cầu, tuy nhiên, hình dạng chính xác có thể thay đổi tùy thuộc vào giống cam, điều kiện sinh trưởng và giai đoạn phát triển.  Một số quả cam có thể hơi dẹt ở hai đầu hoặc hơi bầu dục.  Nhưng nhìn chung, hình cầu là mô tả chính xác nhất về hình dạng của một quả cam.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test prompt with vector search and HYDE search\n",
    "def test_prompt_with_search(query, collection, columns_to_answer, number_docs_retrieval=2):\n",
    "    # Vector search + LLM\n",
    "    metadatas_vector, retrieved_data_vector = vector_search(query, collection, columns_to_answer, number_docs_retrieval)\n",
    "    enhanced_prompt_vector = f\"Bạn là chuyên gia về tư vấn về trái cây, nên đưa ra câu trả lời liên quan đến trái cây: {query}. Dữ liệu được lấy: {retrieved_data_vector}\"\n",
    "    response_vector = modelai.generate_content(enhanced_prompt_vector)\n",
    "\n",
    "    if hasattr(response_vector, 'text'):\n",
    "        response_vector = response_vector.text\n",
    "\n",
    "    print(\"\\n--- Vector Search + LLM ---\")\n",
    "    print(\"Retrieved Data:\", retrieved_data_vector)\n",
    "    print(\"LLM Response:\", response_vector)\n",
    "\n",
    "    # HYDE search + LLM\n",
    "    metadatas_hyde, retrieved_data_hyde = hyde_search(encoder_model, query, collection, columns_to_answer, number_docs_retrieval)\n",
    "    enhanced_prompt_hyde = f\"Bạn là chuyên gia về tư vấn về trái cây, nên đưa ra câu trả lời liên quan đến trái cây: {query}. Dữ liệu được lấy: {retrieved_data_hyde}\"\n",
    "    response_hyde = modelai.generate_content(enhanced_prompt_hyde)\n",
    "\n",
    "    if hasattr(response_hyde, 'text'):\n",
    "        response_hyde = response_hyde.text\n",
    "\n",
    "    print(\"\\n--- HYDE Search + LLM ---\")\n",
    "    print(\"Retrieved Data:\", retrieved_data_hyde)\n",
    "    print(\"LLM Response:\", response_hyde)\n",
    "\n",
    "# Define test parameters\n",
    "test_query = \"quả cam có hình gì\"\n",
    "columns_to_answer = [\"chunk\", \"Câu hỏi\", \"Câu trả lời\"]\n",
    "number_docs_retrieval = 2\n",
    "\n",
    "# Run the test\n",
    "test_prompt_with_search(test_query, collection, columns_to_answer, number_docs_retrieval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
